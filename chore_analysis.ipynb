{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesure de la synchronisation de mouvements de danse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre du projet de reconnaissance visuelle, nous aimerions comparer les mouvements de danseurs différents. En particulier, nous aimerons savoir si deux danseurs sont synchrones dans leurs mouvements pour une chorégraphie donnée. Cela nécessite un prétraitement de chaque frame d'une vidéo, puis d'identifier les différentes parties du corps de chaque danseurs, et enfin d'analyser les mouvements de chacune de ces parties. Dans ce notebook, nous allons traiter chacune de ces parties, en évitant au maximum le recours aux méthodes d'apprentissage automatique, mais en se fixant des contraintes sur le format de la vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_video_path = \"data/chore_jo_wa.mp4\"\n",
    "processed_video_path = \"data/chore_jo_wa_processed.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Prétraitement des données: suppression du fond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.filters import gaussian\n",
    "from scipy import ndimage\n",
    "from skimage.segmentation import active_contour\n",
    "from skimage.draw import polygon\n",
    "from IPython.display import clear_output\n",
    "from time import sleep, time\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Loading Video</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(vidFile):\n",
    "    '''\n",
    "    Reads the video\n",
    "    :param vidFile: Video file\n",
    "    :return: video sequence, frame rate, width & height of video frames\n",
    "    '''\n",
    "    print('Load video')\n",
    "    vid = cv2.VideoCapture(vidFile)\n",
    "    fr = vid.get(cv2.CAP_PROP_FPS)  # frame rate\n",
    "    len = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    vidWidth = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    vidHeight = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # save video as stack of images\n",
    "    video_stack = np.empty((len, vidHeight, vidWidth, 3))\n",
    "\n",
    "    for x in range(len):\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        video_stack[x] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        filename = \"./data/Images/\" + vidFile[7:-4] + \"_\" + str(x) + \".jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "\n",
    "    vid.release()\n",
    "\n",
    "    return video_stack, fr, vidWidth, vidHeight\n",
    "\n",
    "def save_video(fr, video, output_file) : \n",
    "    # Define the video codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can also use other codecs such as 'XVID', 'MJPG', 'DIVX', etc.\n",
    "    output_video = cv2.VideoWriter(output_file, fourcc, fr, (video[0].shape[1], video[0].shape[0]))  # Adjust FPS as needed (e.g., 25 FPS)\n",
    "\n",
    "    # Write cropped_video to the video\n",
    "    for frame in video:\n",
    "        output_video.write(cv2.cvtColor(np.uint8(frame), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Release the video writer\n",
    "    output_video.release()\n",
    "\n",
    "    print(\"Video saved successfully.\")\n",
    "\n",
    "def display(frames_list) :\n",
    "    \n",
    "    ln = len(frames_list)\n",
    "    nb_frames = len(frames_list[0])\n",
    "    print(ln, nb_frames)\n",
    "\n",
    "    for i in range(nb_frames) : \n",
    "        \n",
    "        fig, axs = plt.subplots(1, ln)\n",
    "        \n",
    "        for j in range(ln) :\n",
    "            if frames_list[j][i].max()>1 : \n",
    "                axs[j].imshow(frames_list[j][i].astype(int)) \n",
    "            else : \n",
    "                axs[j].imshow(frames_list[j][i]) \n",
    "                \n",
    "        plt.show()\n",
    "        # sleep(0.001)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_stack, fr, vidWidth, vidHeight = load_video(original_video_path)\n",
    "video_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Background Substraction with actif contours method</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mog(video) :\n",
    "    \n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "    masks = []\n",
    "    i = 0\n",
    "    while(1):\n",
    "        frame = video[i]\n",
    "\n",
    "        fgmask = fgbg.apply(frame)\n",
    "        masks.append(fgmask)\n",
    "\n",
    "        if i<len(video)-1 :     \n",
    "            i += 1\n",
    "        else : \n",
    "            break\n",
    "        \n",
    "    return masks \n",
    "\n",
    "def motion_difference_masks(video) :\n",
    "    \n",
    "    diff_masks = []\n",
    "\n",
    "    for i in range(len(video)-1) :\n",
    "        \n",
    "        diff_masks.append(np.uint8(255*(np.sum(np.abs(video[i]-video[i+1]), axis=2)>20)))\n",
    "        \n",
    "    diff_masks.append(diff_masks[-1])\n",
    "    return diff_masks\n",
    "\n",
    "def filter_masks(masks, kernel_size=4) :\n",
    "    height, width = masks[0].shape\n",
    "    num_frames = len(masks)\n",
    "    filtered_masks = np.zeros((num_frames, height, width), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(len(masks)) : \n",
    "        filtered_masks[i] = signal.convolve2d(filtered_masks[i], np.ones((10, 10)), mode='same')\n",
    "        filtered_masks[i] = (filtered_masks[i]>0.5).astype(int)\n",
    "\n",
    "    # Iterate over each pixel position\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            # Extract pixel values across all frames\n",
    "            pixel_values = masks[:, i, j]\n",
    "\n",
    "            filtered_masks[:, i, j] = ndimage.median_filter(pixel_values, size=kernel_size)\n",
    "\n",
    "    return filtered_masks\n",
    "\n",
    "def fit_active_contour(masks) : \n",
    "    \n",
    "    \n",
    "    s = np.linspace(0, 2*np.pi, 1000)\n",
    "    r = 400 + 800*np.sin(s)\n",
    "    c = 200 + 800*np.cos(s)\n",
    "    init = np.array([r, c]).T\n",
    "    \n",
    "    contours = []\n",
    "    for i in range(len(masks)) :\n",
    "        if not (i%1) : \n",
    "            start_tm = time()\n",
    "            snake = active_contour(gaussian(masks[i], 5, preserve_range=False),\n",
    "                                init, alpha=0.03, beta=1, gamma=0.001)\n",
    "            contours.append(snake)\n",
    "            # init = deepcopy(snake)\n",
    "            print(i, 'Execution time : ', time()-start_tm) \n",
    "        else : \n",
    "            contours.append(contours[-1])\n",
    "        if i<10 : # Display the first 10 frames\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))\n",
    "        ax.imshow(masks[i].astype(int))\n",
    "        # ax.imshow(masks[i], cmap=plt.cm.gray)\n",
    "        ax.plot(contours[i][:, 1], contours[i][:, 0], '-b', lw=3)\n",
    "        ax.set_xticks([]), ax.set_yticks([])\n",
    "        ax.axis([0, masks[i].shape[1], masks[i].shape[0], 0])\n",
    "    \n",
    "\n",
    "        plt.show()\n",
    "    return contours\n",
    "\n",
    "def crop_video(video, contours) : \n",
    "    \n",
    "    croped_video = []\n",
    "    \n",
    "    for i in range(len(video)) : \n",
    "        img = video[i].astype(int)\n",
    "        # poly = np.roll(snake, 1, axis=1).astype(int).reshape(-1, 1, 2) \n",
    "        poly = contours[i].astype(int)\n",
    "\n",
    "        rr, cc = polygon(poly[:, 0], poly[:, 1], img.shape)\n",
    "\n",
    "        masked_img = 255*np.ones(img.shape)\n",
    "        for i in range(3) : \n",
    "            masked_img[rr, cc, i] = img[rr, cc, i]\n",
    "            \n",
    "        croped_video.append(masked_img)\n",
    "        \n",
    "    return croped_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(video) : \n",
    "    \n",
    "    # Compute the motion masks \n",
    "    diff_masks = motion_difference_masks(video)\n",
    "    \n",
    "    # Fit actif contours in each frames's mask \n",
    "    contours = fit_active_contour(diff_masks)    # This step can be optmized by fitting the countours 1/2 frames if motion not so fast \n",
    "    \n",
    "    # Crop the video according to the found masks \n",
    "    cropped_video = crop_video(video, contours)\n",
    "    \n",
    "    return cropped_video, contours, diff_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_video, contours, diff_masks =  preprocess(video_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_video = crop_video(video_stack, contours)\n",
    "\n",
    "save_video(fr, cropped_video, processed_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5) : \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 7))\n",
    "    ax[0].imshow(cropped_video[i].astype(int))\n",
    "    # ax.imshow(cropped_video[i], cmap=plt.cm.gray)\n",
    "    ax[0].plot(contours[i][:, 1], contours[i][:, 0], '-b', lw=3)\n",
    "    ax[0].set_xticks([]), ax[0].set_yticks([])\n",
    "    ax[0].axis([0, cropped_video[i].shape[1], cropped_video[i].shape[0], 0])\n",
    "    \n",
    "    ax[1].imshow(diff_masks[i])\n",
    "    # ax[2].imshow(filtered_masks[:2][i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Reconnaissance des parties du corps et analyse du mouvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le danseur bien rogné, nous nous plaçons dans le cadre bien précis où les parties du corps à identifier sont identifiés par le port d'un objet de couleurs distinctes. Dans notre cas, le port d'un chapeau kaki pour identifier le haut de la tête, et respectivement d'une chaussette rose et d'une chaussure rouge pour le pied gauche et le pied droit.  \n",
    "Nous avons instauré ces contraintes dû à la difficulté de reconnaitre les parties du corps sans l'aide d'algorithme de deeplearning. En particulier, même en définissant les pieds comme étant les points les plus bas du rognage, le pied droit n'est pas nécessairement à droite et le pied gauche n'est pas nécessairement à gauche. Ces généralités sont d'autant plus fausses pour des chorégraphies de danse riches en positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code commenté ci-dessous a servi à identifier les fenêtres de couleur HSV des différents objets. Nous avons trouvé HSV ici plus adapté que RGB par exemple puisque la teinte de l'objet n'est pas supposée porter de variations importantes au cours de la vidéo, tandis que le contraste et la luminosité doivent varier davantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def apply_hsv_filter(frame, lower_bound, upper_bound):\n",
    "#     hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "#     mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "    \n",
    "#     # Créer un cadre vert\n",
    "#     green_frame = np.zeros_like(frame)\n",
    "#     green_frame[:, :] = (0, 255, 0)  # BGR pour le vert\n",
    "    \n",
    "#     # Utiliser le masque pour combiner le cadre vert et le cadre filtré\n",
    "#     filtered_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "#     final_frame = cv2.bitwise_or(filtered_frame, cv2.bitwise_and(green_frame, green_frame, mask=~mask))\n",
    "    \n",
    "#     return final_frame\n",
    "\n",
    "# def display_frames(original_frame, filtered_frame):\n",
    "#     # Concaténer les images horizontalement\n",
    "#     output_frame = np.hstack((original_frame, filtered_frame))\n",
    "#     # Afficher l'image\n",
    "#     plt.imshow(cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def process_video(input_file, lower_bound, upper_bound, mode='static'):\n",
    "#     cap = cv2.VideoCapture(input_file)\n",
    "    \n",
    "#     if mode == 'static':\n",
    "#         for _ in range(5):\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             filtered_frame = apply_hsv_filter(frame, lower_bound, upper_bound)\n",
    "#             display_frames(frame, filtered_frame)\n",
    "#     elif mode == 'video':\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             filtered_frame = apply_hsv_filter(frame, lower_bound, upper_bound)\n",
    "#             display_frames(frame, filtered_frame)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "    \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Paramètres HSV pour le filtrage\n",
    "# lower_bound_fuchsia = np.array([140, 50, 100])\n",
    "# upper_bound_fuchsia = np.array([170, 255, 255])\n",
    "\n",
    "# lower_bound_red = np.array([170, 50, 100])\n",
    "# upper_bound_red = np.array([179, 255, 255])\n",
    "\n",
    "# lower_bound_brown = np.array([10, 70, 50])\n",
    "# upper_bound_brown = np.array([30, 255, 100])\n",
    "\n",
    "# # Mode d'exécution (statique ou vidéo)\n",
    "# mode = 'video' \n",
    "\n",
    "# # Appel de la fonction pour traiter la vidéo avec les paramètres spécifiés\n",
    "# process_video(processed_video_path, lower_bound_fuchsia, upper_bound_fuchsia, mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une première classe générique pour représenter chaque objet. Chaque objet est défini par le résultat du masque appliqué pour retrouver les pixels définis dans une fenêtre de couleurs HSV. Nous commençons par trouver les contours de ces zones là. Si la frame est très bruitée et contient des pixels \"aléatoires\" des teintes de notre fenêtre, ils seront éliminés puisque notre fonction find_objects ne prend en compte que le plus grand contour. Le barycentre est alors calculé pour cette zone, et fait office de coordonnées pour notre objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter:\n",
    "    def __init__(self, lower_bound, upper_bound, min_area=100, k=1):\n",
    "        self.lower_bound = np.array(lower_bound)\n",
    "        self.upper_bound = np.array(upper_bound)\n",
    "        self.min_area = min_area\n",
    "        self.k = k\n",
    "\n",
    "    def convert_to_hsv(self, frame):\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    def apply_mask(self, frame):\n",
    "        hsv = self.convert_to_hsv(frame)\n",
    "        mask = cv2.inRange(hsv, self.lower_bound, self.upper_bound)\n",
    "        mask = self.filter_small_masks(mask)\n",
    "        _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        return binary_mask\n",
    "\n",
    "\n",
    "    def filter_small_masks(self, mask):\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        filtered_mask = np.zeros_like(mask)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) > self.min_area:\n",
    "                cv2.drawContours(filtered_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        return filtered_mask\n",
    "\n",
    "    def display_result(self, original_frame, result_frame, display_mode='static'):\n",
    "        if display_mode == 'video':\n",
    "            # Pour le mode vidéo, combine et montre l'image directement sans convertir en RGB\n",
    "            combined = np.hstack((original_frame, result_frame))\n",
    "            cv2.imshow('Result', combined)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "        elif display_mode == 'static':\n",
    "            # Pour le mode statique (utilisé avec matplotlib), convertit en RGB\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].imshow(cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB))\n",
    "            ax[0].set_title('Original Frame')\n",
    "            ax[0].axis('off')\n",
    "            ax[1].imshow(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "            ax[1].set_title('Mask Applied')\n",
    "            ax[1].axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    def find_objects(self, mask):\n",
    "        \"\"\"\n",
    "        Trouve jusqu'à k objets basés sur les contours dans le masque.\n",
    "        Retourne les barycentres des k plus grands contours.\n",
    "        \"\"\"\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:self.k]\n",
    "        barycentres = []\n",
    "        for contour in sorted_contours:\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                barycentres.append((cx, cy))\n",
    "        return barycentres\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit alors des classes pour chacun des objets, avec des fenêtres de couleurs différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeftShoeSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[170, 50, 100], upper_bound=[179, 255, 255], k=1)\n",
    "\n",
    "class RightShoeSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[140, 50, 80], upper_bound=[170, 255, 255], k=1)\n",
    "\n",
    "class CapSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[10, 110, 50], upper_bound=[30, 255, 100], k=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit ensuite une classe ObjectTracker pour stocker les positions, vitesses et accélérations de chaque objet. \n",
    "Nous définissons un impact d'action par un changement brusque de l'accélération. De plus, nous pensons que la limite définissant cet impact dépend de la partie du corps: un mouvement de pied peut-être beaucoup plus brusque qu'un mouvement de tête. Chaque objet aura donc sa propre limite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    def __init__(self, fps, tracker_threshold=500.0):\n",
    "        self.fps = fps\n",
    "        self.positions = []  # Liste des tuples (x, y)\n",
    "        self.velocities = []  # Liste des tuples (vx, vy)\n",
    "        self.accelerations = []  # Liste des tuples (ax, ay)\n",
    "        self.previous_acceleration = None\n",
    "        self.tracker_threshold = tracker_threshold\n",
    "\n",
    "    def update_position(self, position):\n",
    "        \"\"\"Mise à jour de la position de l'objet avec une position tuple (x, y) directe.\"\"\"\n",
    "        if position:\n",
    "            self.positions.append(position)\n",
    "        else:\n",
    "            self.positions.append((0, 0))\n",
    "    \n",
    "    def get_latest_position(self):\n",
    "        \"\"\"Retourne la dernière position connue de l'objet.\"\"\"\n",
    "        if self.positions:\n",
    "            return self.positions[-1]\n",
    "        else:\n",
    "            return None  # Retourne None si aucune position n'a été enregistrée\n",
    "\n",
    "    def calculate_velocity(self):\n",
    "        \"\"\"Calcule la vitesse (vx, vy) entre les positions consécutives.\"\"\"\n",
    "        if len(self.positions) > 1:\n",
    "            dx = self.positions[-1][0] - self.positions[-2][0]\n",
    "            dy = self.positions[-1][1] - self.positions[-2][1]\n",
    "            # Calculer la vitesse comme un vecteur (vx, vy)\n",
    "            vx = dx * self.fps\n",
    "            vy = dy * self.fps\n",
    "            self.velocities.append((vx, vy))\n",
    "        else:\n",
    "            self.velocities.append((0, 0))\n",
    "\n",
    "    def calculate_acceleration(self):\n",
    "        \"\"\"Calcule l'accélération (ax, ay) entre les vitesses consécutives.\"\"\"\n",
    "        if len(self.velocities) > 1:\n",
    "            dvx = self.velocities[-1][0] - self.velocities[-2][0]\n",
    "            dvy = self.velocities[-1][1] - self.velocities[-2][1]\n",
    "            # Calculer l'accélération comme un vecteur (ax, ay)\n",
    "            ax = dvx * self.fps\n",
    "            ay = dvy * self.fps\n",
    "            self.accelerations.append((ax, ay))\n",
    "        else:\n",
    "            self.accelerations.append((0, 0))\n",
    "\n",
    "    def get_latest_velocity(self):\n",
    "        \"\"\"Retourne la dernière vitesse connue de l'objet.\"\"\"\n",
    "        if self.velocities:\n",
    "            return self.velocities[-1]\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    def get_latest_acceleration(self):\n",
    "        \"\"\"Retourne la dernière accélération connue de l'objet.\"\"\"\n",
    "        if self.accelerations:\n",
    "            return self.accelerations[-1]\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    def smooth_velocity(self, window_size=10):\n",
    "        \"\"\"Lisse la vitesse en utilisant une moyenne mobile.\"\"\"\n",
    "        smoothed_velocities = []\n",
    "        for i in range(len(self.velocities)):\n",
    "            # Déterminer la fenêtre pour la moyenne mobile\n",
    "            start_index = max(0, i - window_size + 1)\n",
    "            end_index = i + 1\n",
    "            window = self.velocities[start_index:end_index]\n",
    "            \n",
    "            # Calculer la moyenne dans la fenêtre\n",
    "            vx_avg = sum(v[0] for v in window) / len(window)\n",
    "            vy_avg = sum(v[1] for v in window) / len(window)\n",
    "            smoothed_velocities.append((vx_avg, vy_avg))\n",
    "        \n",
    "        return smoothed_velocities\n",
    "\n",
    "    def smooth_acceleration(self, window_size=20):\n",
    "        \"\"\"Lisse la vitesse en utilisant une moyenne mobile.\"\"\"\n",
    "        smoothed_acceleration = []\n",
    "        for i in range(len(self.accelerations)):\n",
    "            # Déterminer la fenêtre pour la moyenne mobile\n",
    "            start_index = max(0, i - window_size + 1)\n",
    "            end_index = i + 1\n",
    "            window = self.accelerations[start_index:end_index]\n",
    "            \n",
    "            # Calculer la moyenne dans la fenêtre\n",
    "            vx_avg = sum(v[0] for v in window) / len(window)\n",
    "            vy_avg = sum(v[1] for v in window) / len(window)\n",
    "            smoothed_acceleration.append((vx_avg, vy_avg))\n",
    "        \n",
    "        return smoothed_acceleration\n",
    "\n",
    "    def detect_impact(self):\n",
    "        if self.previous_acceleration is None:\n",
    "            self.previous_acceleration = self.get_latest_acceleration()\n",
    "            return False\n",
    "\n",
    "        current_acceleration = self.get_latest_acceleration()\n",
    "        diff = np.array(current_acceleration) - np.array(self.previous_acceleration)\n",
    "        impact_detected = np.linalg.norm(diff) > self.tracker_threshold\n",
    "        self.previous_acceleration = current_acceleration\n",
    "\n",
    "        return impact_detected\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des paramètres\n",
    "fps = 30\n",
    "cap = cv2.VideoCapture(processed_video_path)\n",
    "\n",
    "# Création des segmenteurs\n",
    "left_shoe_segmenter = LeftShoeSegmenter() # Exemple\n",
    "right_shoe_segmenter = RightShoeSegmenter() # Exemple\n",
    "cap_segmenter = CapSegmenter()\n",
    "\n",
    "# Création des trackers correspondants\n",
    "left_shoe_tracker = ObjectTracker(fps=fps)\n",
    "right_shoe_tracker = ObjectTracker(fps=fps)\n",
    "cap_tracker = ObjectTracker(fps=fps, tracker_threshold=800.0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions suivantes servent à dessiner les positions et les vecteurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_positions(frame, positions, color=(100, 150, 30)):\n",
    "    \"\"\"\n",
    "    Dessine des cercles aux positions fournies sur la frame.\n",
    "    \"\"\"\n",
    "    for (x, y) in positions:\n",
    "        cv2.circle(frame, (x, y), radius=30, color=color, thickness=-1)\n",
    "    return frame\n",
    "\n",
    "def draw_vectors(frame, position, velocity, acceleration, color_velocity=(255, 0, 0), color_acceleration=(0, 0, 255)):\n",
    "    \"\"\"\n",
    "    Dessine les vecteurs vitesse et accélération pour un objet sur la frame.\n",
    "    \n",
    "    :param frame: L'image sur laquelle dessiner.\n",
    "    :param position: La position actuelle de l'objet (x, y).\n",
    "    :param velocity: La vitesse de l'objet sous forme de tuple (vx, vy).\n",
    "    :param acceleration: L'accélération de l'objet sous forme de tuple (ax, ay).\n",
    "    :param color_velocity: La couleur du vecteur vitesse.\n",
    "    :param color_acceleration: La couleur du vecteur accélération.\n",
    "    \"\"\"\n",
    "    if position and velocity:\n",
    "        end_point_velocity = (int(position[0] + velocity[0]), int(position[1] + velocity[1]))\n",
    "        cv2.arrowedLine(frame, position, end_point_velocity, color_velocity, 2, tipLength=0.5)\n",
    "    \n",
    "    if position and acceleration:\n",
    "        end_point_acceleration = (int(position[0] + acceleration[0]), int(position[1] + acceleration[1]))\n",
    "        cv2.arrowedLine(frame, position, end_point_acceleration, color_acceleration, 2, tipLength=0.5)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def draw_impact(frame, position, color=(0, 0, 255), radius=30):\n",
    "    \"\"\"\n",
    "    Dessine un point d'impact sur la frame.\n",
    "\n",
    "    Args:\n",
    "        frame (np.array): La frame sur laquelle dessiner.\n",
    "        position (tuple): La position de l'impact.\n",
    "        color (tuple, optional): La couleur du point d'impact. Par défaut à rouge.\n",
    "        radius (int, optional): Le rayon du point d'impact. Par défaut à 5.\n",
    "    \"\"\"\n",
    "    cv2.circle(frame, position, radius, color, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On traite alors la vidéo. Notons que nous avons ici décidé de lisser les vecteurs de vitesse et d'accélération en faisant une moyenne glissée pour aténuer les variations dues au bruit des zones de couleurs. Enfin, le résultat pourra être affiché sous forme de vidéos ou pour les premières frames de la vidéo avec le paramètre display_mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, segmenters_and_trackers, display_mode='static'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or (display_mode == 'static' and frame_count >= 10):\n",
    "            break\n",
    "\n",
    "        combined_masks = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Mise à jour des positions, calcul et lissage des vitesses et accélérations pour chaque tracker\n",
    "        for segmenter, tracker in segmenters_and_trackers:\n",
    "            mask = segmenter.apply_mask(frame)\n",
    "            positions = segmenter.find_objects(mask)\n",
    "            for pos in positions:\n",
    "                tracker.update_position(pos)\n",
    "            tracker.calculate_velocity()\n",
    "            tracker.calculate_acceleration()\n",
    "\n",
    "        # Après avoir calculé les vitesses et accélérations, appliquez le lissage\n",
    "        for _, tracker in segmenters_and_trackers:\n",
    "            tracker.velocities = tracker.smooth_velocity()  # Remplacer par la méthode de lissage appropriée\n",
    "        for _, tracker in segmenters_and_trackers:\n",
    "            tracker.accelerations = tracker.smooth_acceleration()\n",
    "        \n",
    "        # Conversion du masque combiné et préparation de la frame pour le dessin\n",
    "        combined_masks_bgr = cv2.cvtColor(combined_masks, cv2.COLOR_GRAY2BGR)\n",
    "        combined_frame = np.hstack((frame, combined_masks_bgr))\n",
    "\n",
    "        # Dessin des positions et vecteurs lissés\n",
    "        for segmenter, tracker in segmenters_and_trackers:\n",
    "            latest_position = tracker.get_latest_position()\n",
    "            if latest_position:\n",
    "                adjusted_position = (latest_position[0] + frame.shape[1], latest_position[1])\n",
    "                draw_positions(combined_frame, [adjusted_position], color=(0, 255, 0))\n",
    "\n",
    "                velocity = tracker.get_latest_velocity()\n",
    "                acceleration = tracker.get_latest_acceleration()\n",
    "                \n",
    "                # Ajustement et dessin des vecteurs sur la frame combinée\n",
    "                velocity_adjusted = (int(velocity[0] * 5), int(velocity[1] * 5))\n",
    "                acceleration_adjusted = (int(acceleration[0] * 0.3), int(acceleration[1] * 0.3))\n",
    "                draw_vectors(combined_frame, adjusted_position, velocity_adjusted, acceleration_adjusted, color_velocity=(255, 0, 0), color_acceleration=(0, 255, 255))\n",
    "                \n",
    "                # Détection des points d'impact et dessin sur la frame combinée\n",
    "                if tracker.detect_impact():\n",
    "                    draw_impact(combined_frame, adjusted_position, color=(0, 0, 255))\n",
    "\n",
    "        # Affichage ou sauvegarde de la frame traitée\n",
    "        if display_mode == 'video':\n",
    "            cv2.imshow('Tracked Objects', combined_frame)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        elif display_mode == 'static' and frame_count < 10:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(cv2.cvtColor(combined_frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    for _ in range(4): cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associez chaque segmenteur à son tracker\n",
    "segmenters_and_trackers = [(left_shoe_segmenter, left_shoe_tracker), (right_shoe_segmenter, right_shoe_tracker), (cap_segmenter, cap_tracker)]\n",
    "\n",
    "# Processus d'exemple\n",
    "process_video(processed_video_path, segmenters_and_trackers, display_mode='video')  # Ou 'video' pour l'affichage vidéo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
