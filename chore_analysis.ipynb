{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesure de la synchronisation de mouvements de danse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre du projet de reconnaissance visuelle, nous aimerions comparer les mouvements de danseurs différents. En particulier, nous aimerons savoir si deux danseurs sont synchrones dans leurs mouvements pour une chorégraphie donnée. Cela nécessite un prétraitement de chaque frame d'une vidéo, puis d'identifier les différentes parties du corps de chaque danseurs, et enfin d'analyser les mouvements de chacune de ces parties. Dans ce notebook, nous allons traiter chacune de ces parties, en évitant au maximum le recours aux méthodes d'apprentissage automatique, mais en se fixant des contraintes sur le format de la vidéo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Prétraitement des données: suppression du fond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Reconnaissance des parties du corps et analyse du mouvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le danseur bien rogné, nous nous plaçons dans le cadre bien précis où les parties du corps à identifier sont identifiés par le port d'un objet de couleurs distinctes. Dans notre cas, le port d'un chapeau kaki pour identifier le haut de la tête, et respectivement d'une chaussette rose et d'une chaussure rouge pour le pied gauche et le pied droit.  \n",
    "Nous avons instauré ces contraintes dû à la difficulté de reconnaitre les parties du corps sans l'aide d'algorithme de deeplearning. En particulier, même en définissant les pieds comme étant les points les plus bas du rognage, le pied droit n'est pas nécessairement à droite et le pied gauche n'est pas nécessairement à gauche. Ces généralités sont d'autant plus fausses pour des chorégraphies de danse riches en positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_path = 'data/me2.mov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code commenté ci-dessous a servi à identifier les fenêtres de couleur HSV des différents objets. Nous avons trouvé HSV ici plus adapté que RGB par exemple puisque la teinte de l'objet n'est pas supposée porter de variations importantes au cours de la vidéo, tandis que le contraste et la luminosité doivent varier davantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def apply_hsv_filter(frame, lower_bound, upper_bound):\n",
    "#     hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "#     mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "#     filtered_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "#     return filtered_frame\n",
    "\n",
    "# def display_frames(original_frame, filtered_frame):\n",
    "#     # Concaténer les images horizontalement\n",
    "#     output_frame = np.hstack((original_frame, filtered_frame))\n",
    "#     # Afficher l'image\n",
    "#     plt.imshow(cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def process_video(input_file, lower_bound, upper_bound, mode='static'):\n",
    "#     cap = cv2.VideoCapture(input_file)\n",
    "    \n",
    "#     if mode == 'static':\n",
    "#         for _ in range(5):\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             filtered_frame = apply_hsv_filter(frame, lower_bound, upper_bound)\n",
    "#             display_frames(frame, filtered_frame)\n",
    "#     elif mode == 'video':\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             filtered_frame = apply_hsv_filter(frame, lower_bound, upper_bound)\n",
    "#             display_frames(frame, filtered_frame)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "    \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Paramètres HSV pour le filtrage\n",
    "# lower_bound_fuchsia = np.array([140, 100, 100])\n",
    "# upper_bound_fuchsia = np.array([170, 255, 255])\n",
    "\n",
    "# lower_bound_red = np.array([170, 50, 100])\n",
    "# upper_bound_red = np.array([179, 255, 255])\n",
    "\n",
    "# lower_bound_brown = np.array([10, 70, 50])\n",
    "# upper_bound_brown = np.array([30, 255, 255])\n",
    "\n",
    "# # Mode d'exécution (statique ou vidéo)\n",
    "# mode = 'static' \n",
    "\n",
    "# # Appel de la fonction pour traiter la vidéo avec les paramètres spécifiés\n",
    "# process_video(video_path, lower_bound_brown, upper_bound_brown, mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons une première classe générique pour représenter chaque objet. Chaque objet est défini par le résultat du masque appliqué pour retrouver les pixels définis dans une fenêtre de couleurs HSV. Nous commençons par trouver les contours de ces zones là. Si la frame est très bruitée et contient des pixels \"aléatoires\" des teintes de notre fenêtre, ils seront éliminés puisque notre fonction find_objects ne prend en compte que le plus grand contour. Le barycentre est alors calculé pour cette zone, et fait office de coordonnées pour notre objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter:\n",
    "    def __init__(self, lower_bound, upper_bound, min_area=100, k=1):\n",
    "        self.lower_bound = np.array(lower_bound)\n",
    "        self.upper_bound = np.array(upper_bound)\n",
    "        self.min_area = min_area\n",
    "        self.k = k\n",
    "\n",
    "    def convert_to_hsv(self, frame):\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    def apply_mask(self, frame):\n",
    "        hsv = self.convert_to_hsv(frame)\n",
    "        mask = cv2.inRange(hsv, self.lower_bound, self.upper_bound)\n",
    "        mask = self.filter_small_masks(mask)\n",
    "        _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        return binary_mask\n",
    "\n",
    "\n",
    "    def filter_small_masks(self, mask):\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        filtered_mask = np.zeros_like(mask)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) > self.min_area:\n",
    "                cv2.drawContours(filtered_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        return filtered_mask\n",
    "\n",
    "    def display_result(self, original_frame, result_frame, display_mode='static'):\n",
    "        if display_mode == 'video':\n",
    "            # Pour le mode vidéo, combine et montre l'image directement sans convertir en RGB\n",
    "            combined = np.hstack((original_frame, result_frame))\n",
    "            cv2.imshow('Result', combined)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "        elif display_mode == 'static':\n",
    "            # Pour le mode statique (utilisé avec matplotlib), convertit en RGB\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].imshow(cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB))\n",
    "            ax[0].set_title('Original Frame')\n",
    "            ax[0].axis('off')\n",
    "            ax[1].imshow(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "            ax[1].set_title('Mask Applied')\n",
    "            ax[1].axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    def find_objects(self, mask):\n",
    "        \"\"\"\n",
    "        Trouve jusqu'à k objets basés sur les contours dans le masque.\n",
    "        Retourne les barycentres des k plus grands contours.\n",
    "        \"\"\"\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:self.k]\n",
    "        barycentres = []\n",
    "        for contour in sorted_contours:\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                barycentres.append((cx, cy))\n",
    "        return barycentres\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit alors des classes pour chacun des objets, avec des fenêtres de couleurs différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeftShoeSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[170, 50, 100], upper_bound=[179, 255, 255], k=1)\n",
    "\n",
    "class RightShoeSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[140, 100, 100], upper_bound=[170, 255, 255], k=1)\n",
    "\n",
    "class CapSegmenter(Segmenter):\n",
    "    def __init__(self):\n",
    "        super().__init__(lower_bound=[10, 70, 50], upper_bound=[30, 255, 255], k=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit ensuite une classe ObjectTracker pour stocker les positions, vitesses et accélérations de chaque objet.  \n",
    "Nous testons diverses techniques pour détecter les impacts d'un mouvement de danse, par un changement drastique du vecteur accélération, ou d'un changement de direction du vecteur vitesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    def __init__(self, fps):\n",
    "        self.fps = fps\n",
    "        self.positions = []  # Liste des tuples (x, y)\n",
    "        self.velocities = []  # Liste des tuples (vx, vy)\n",
    "        self.accelerations = []  # Liste des tuples (ax, ay)\n",
    "\n",
    "    def update_position(self, position):\n",
    "        \"\"\"Mise à jour de la position de l'objet avec une position tuple (x, y) directe.\"\"\"\n",
    "        if position:\n",
    "            self.positions.append(position)\n",
    "        else:\n",
    "            self.positions.append((0, 0))\n",
    "    \n",
    "    def get_latest_position(self):\n",
    "        \"\"\"Retourne la dernière position connue de l'objet.\"\"\"\n",
    "        if self.positions:\n",
    "            return self.positions[-1]\n",
    "        else:\n",
    "            return None  # Retourne None si aucune position n'a été enregistrée\n",
    "\n",
    "    def calculate_velocity(self):\n",
    "        \"\"\"Calcule la vitesse (vx, vy) entre les positions consécutives.\"\"\"\n",
    "        if len(self.positions) > 1:\n",
    "            dx = self.positions[-1][0] - self.positions[-2][0]\n",
    "            dy = self.positions[-1][1] - self.positions[-2][1]\n",
    "            # Calculer la vitesse comme un vecteur (vx, vy)\n",
    "            vx = dx * self.fps\n",
    "            vy = dy * self.fps\n",
    "            self.velocities.append((vx, vy))\n",
    "        else:\n",
    "            self.velocities.append((0, 0))\n",
    "\n",
    "    def calculate_acceleration(self):\n",
    "        \"\"\"Calcule l'accélération (ax, ay) entre les vitesses consécutives.\"\"\"\n",
    "        if len(self.velocities) > 1:\n",
    "            dvx = self.velocities[-1][0] - self.velocities[-2][0]\n",
    "            dvy = self.velocities[-1][1] - self.velocities[-2][1]\n",
    "            # Calculer l'accélération comme un vecteur (ax, ay)\n",
    "            ax = dvx * self.fps\n",
    "            ay = dvy * self.fps\n",
    "            self.accelerations.append((ax, ay))\n",
    "        else:\n",
    "            self.accelerations.append((0, 0))\n",
    "\n",
    "    def get_latest_velocity(self):\n",
    "        \"\"\"Retourne la dernière vitesse connue de l'objet.\"\"\"\n",
    "        if self.velocities:\n",
    "            return self.velocities[-1]\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    def get_latest_acceleration(self):\n",
    "        \"\"\"Retourne la dernière accélération connue de l'objet.\"\"\"\n",
    "        if self.accelerations:\n",
    "            return self.accelerations[-1]\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    def smooth_velocity(self, window_size=10):\n",
    "        \"\"\"Lisse la vitesse en utilisant une moyenne mobile.\"\"\"\n",
    "        smoothed_velocities = []\n",
    "        for i in range(len(self.velocities)):\n",
    "            # Déterminer la fenêtre pour la moyenne mobile\n",
    "            start_index = max(0, i - window_size + 1)\n",
    "            end_index = i + 1\n",
    "            window = self.velocities[start_index:end_index]\n",
    "            \n",
    "            # Calculer la moyenne dans la fenêtre\n",
    "            vx_avg = sum(v[0] for v in window) / len(window)\n",
    "            vy_avg = sum(v[1] for v in window) / len(window)\n",
    "            smoothed_velocities.append((vx_avg, vy_avg))\n",
    "        \n",
    "        return smoothed_velocities\n",
    "\n",
    "    def smooth_acceleration(self, window_size=20):\n",
    "        \"\"\"Lisse la vitesse en utilisant une moyenne mobile.\"\"\"\n",
    "        smoothed_acceleration = []\n",
    "        for i in range(len(self.accelerations)):\n",
    "            # Déterminer la fenêtre pour la moyenne mobile\n",
    "            start_index = max(0, i - window_size + 1)\n",
    "            end_index = i + 1\n",
    "            window = self.accelerations[start_index:end_index]\n",
    "            \n",
    "            # Calculer la moyenne dans la fenêtre\n",
    "            vx_avg = sum(v[0] for v in window) / len(window)\n",
    "            vy_avg = sum(v[1] for v in window) / len(window)\n",
    "            smoothed_acceleration.append((vx_avg, vy_avg))\n",
    "        \n",
    "        return smoothed_acceleration\n",
    "    \n",
    "    def calculate_acceleration_change(self):\n",
    "        \"\"\"Calcule la différence entre les accélérations consécutives.\"\"\"\n",
    "        acceleration_changes = []\n",
    "        for i in range(1, len(self.accelerations)):\n",
    "            current_acc = self.accelerations[i]\n",
    "            prev_acc = self.accelerations[i-1]\n",
    "            # Calculer la différence vectorielle entre accélérations consécutives\n",
    "            delta_acc = (current_acc[0] - prev_acc[0], current_acc[1] - prev_acc[1])\n",
    "            # Calculer la norme de la différence\n",
    "            delta_acc_norm = np.sqrt(delta_acc[0]**2 + delta_acc[1]**2)\n",
    "            acceleration_changes.append(delta_acc_norm)\n",
    "        return acceleration_changes\n",
    "\n",
    "    def detect_impacts(self, threshold=5000.0):\n",
    "        impacts = []\n",
    "        acceleration_changes = self.calculate_acceleration_change()\n",
    "        for i, change in enumerate(acceleration_changes):\n",
    "            if change > threshold:\n",
    "                # Assurez-vous que l'accès à self.positions est valide.\n",
    "                # Si i + 1 est utilisé pour aligner avec les changements d'accélération, \n",
    "                # il faut s'assurer que cela correspond correctement aux indices des positions.\n",
    "                if i < len(self.positions) - 1:  # S'assurer que l'index est dans la plage\n",
    "                    impacts.append((i + 1, self.positions[i + 1]))  \n",
    "                else:\n",
    "                    # Si l'index dépasse, utilisez le dernier disponible.\n",
    "                    impacts.append((i + 1, self.positions[-1]))  \n",
    "        print(f\"Impacts détectés : {impacts}\")\n",
    "        return impacts\n",
    "\n",
    "    def detect_acceleration_peaks(self, acceleration_threshold=5.0):\n",
    "        \"\"\"Détecte les instants d'impact basés sur des pics d'accélération.\"\"\"\n",
    "        impacts = []\n",
    "        for i in range(1, len(self.accelerations)):\n",
    "            # Calculer la norme de l'accélération actuelle\n",
    "            acceleration_norm = np.sqrt(self.accelerations[i][0]**2 + self.accelerations[i][1]**2)\n",
    "\n",
    "            if acceleration_norm > acceleration_threshold:\n",
    "                impacts.append(i)  # L'indexation commence à 0\n",
    "                \n",
    "        return impacts\n",
    "\n",
    "    def calculate_angle_changes(self):\n",
    "        \"\"\"Calcule les changements d'angle entre les vecteurs de vitesse consécutifs.\"\"\"\n",
    "        angle_changes = []\n",
    "        for i in range(1, len(self.velocities)):\n",
    "            v1 = self.velocities[i-1]\n",
    "            v2 = self.velocities[i]\n",
    "            # Calculer l'angle en radians entre v1 et v2\n",
    "            dot_product = np.dot(v1, v2)\n",
    "            norm_v1 = np.linalg.norm(v1)\n",
    "            norm_v2 = np.linalg.norm(v2)\n",
    "            # Éviter la division par zéro\n",
    "            if norm_v1 == 0 or norm_v2 == 0:\n",
    "                angle = 0\n",
    "            else:\n",
    "                cos_angle = dot_product / (norm_v1 * norm_v2)\n",
    "                # S'assurer que cos_angle reste dans les limites valides pour l'arc cosinus\n",
    "                cos_angle = np.clip(cos_angle, -1, 1)\n",
    "                angle = np.arccos(cos_angle)\n",
    "            \n",
    "            # Convertir l'angle en degrés pour une interprétation plus facile\n",
    "            angle_degrees = np.degrees(angle)\n",
    "            angle_changes.append(angle_degrees)\n",
    "        \n",
    "        return angle_changes\n",
    "\n",
    "    def detect_direction_changes(self, angle_threshold=30, speed_threshold=0.5):\n",
    "        \"\"\"Détecte les instants où le changement d'angle dépasse un seuil donné et la vitesse est au-dessus d'un certain seuil.\"\"\"\n",
    "        impacts = []\n",
    "        angle_changes = self.calculate_angle_changes()\n",
    "        for i in range(len(angle_changes)):\n",
    "            # Vérifier également que la vitesse à cet instant dépasse le seuil minimal pour être considérée comme un mouvement\n",
    "            speed_norm = np.linalg.norm(self.velocities[i+1])  # i+1 car les angles changent sont calculés à partir de la seconde vitesse\n",
    "            if angle_changes[i] > angle_threshold and speed_norm > speed_threshold:\n",
    "                impacts.append(i + 1)  # Ajouter 1 si l'indexation de frames commence à 1\n",
    "                \n",
    "        return impacts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des paramètres\n",
    "fps = 30\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Création des segmenteurs\n",
    "left_shoe_segmenter = LeftShoeSegmenter() # Exemple\n",
    "right_shoe_segmenter = RightShoeSegmenter() # Exemple\n",
    "cap_segmenter = CapSegmenter()\n",
    "\n",
    "# Création des trackers correspondants\n",
    "left_shoe_tracker = ObjectTracker(fps=fps)\n",
    "right_shoe_tracker = ObjectTracker(fps=fps)\n",
    "cap_tracker = ObjectTracker(fps=fps)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions suivantes servent à dessiner les positions et les vecteurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_positions(frame, positions, color=(100, 150, 30)):\n",
    "    \"\"\"\n",
    "    Dessine des cercles aux positions fournies sur la frame.\n",
    "    \"\"\"\n",
    "    for (x, y) in positions:\n",
    "        cv2.circle(frame, (x, y), radius=30, color=color, thickness=-1)\n",
    "    return frame\n",
    "\n",
    "def draw_vectors(frame, position, velocity, acceleration, color_velocity=(255, 0, 0), color_acceleration=(0, 0, 255)):\n",
    "    \"\"\"\n",
    "    Dessine les vecteurs vitesse et accélération pour un objet sur la frame.\n",
    "    \n",
    "    :param frame: L'image sur laquelle dessiner.\n",
    "    :param position: La position actuelle de l'objet (x, y).\n",
    "    :param velocity: La vitesse de l'objet sous forme de tuple (vx, vy).\n",
    "    :param acceleration: L'accélération de l'objet sous forme de tuple (ax, ay).\n",
    "    :param color_velocity: La couleur du vecteur vitesse.\n",
    "    :param color_acceleration: La couleur du vecteur accélération.\n",
    "    \"\"\"\n",
    "    if position and velocity:\n",
    "        end_point_velocity = (int(position[0] + velocity[0]), int(position[1] + velocity[1]))\n",
    "        cv2.arrowedLine(frame, position, end_point_velocity, color_velocity, 2, tipLength=0.5)\n",
    "    \n",
    "    if position and acceleration:\n",
    "        end_point_acceleration = (int(position[0] + acceleration[0]), int(position[1] + acceleration[1]))\n",
    "        cv2.arrowedLine(frame, position, end_point_acceleration, color_acceleration, 2, tipLength=0.5)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On traite alors la vidéo. Notons que nous avons ici décidé de lisser les vecteurs de vitesse et d'accélération en faisant une moyenne glissée pour aténuer les variations dues au bruit des zones de couleurs. Enfin, le résultat pourra être affiché sous forme de vidéos ou pour les premières frames de la vidéo avec le paramètre display_mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, segmenters_and_trackers, display_mode='static'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or (display_mode == 'static' and frame_count >= 10):\n",
    "            break\n",
    "\n",
    "        combined_masks = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Mise à jour des positions, calcul et lissage des vitesses et accélérations pour chaque tracker\n",
    "        for segmenter, tracker in segmenters_and_trackers:\n",
    "            mask = segmenter.apply_mask(frame)\n",
    "            positions = segmenter.find_objects(mask)\n",
    "            for pos in positions:\n",
    "                tracker.update_position(pos)\n",
    "            tracker.calculate_velocity()\n",
    "            tracker.calculate_acceleration()\n",
    "\n",
    "        # Après avoir calculé les vitesses et accélérations, appliquez le lissage\n",
    "        for _, tracker in segmenters_and_trackers:\n",
    "            tracker.velocities = tracker.smooth_velocity()  # Remplacer par la méthode de lissage appropriée\n",
    "              # Remplacer par la méthode de lissage appropriée\n",
    "        for _, tracker in segmenters_and_trackers:\n",
    "            tracker.accelerations = tracker.smooth_acceleration()\n",
    "        \n",
    "        # Conversion du masque combiné et préparation de la frame pour le dessin\n",
    "        combined_masks_bgr = cv2.cvtColor(combined_masks, cv2.COLOR_GRAY2BGR)\n",
    "        combined_frame = np.hstack((frame, combined_masks_bgr))\n",
    "\n",
    "        # Dessin des positions et vecteurs lissés\n",
    "        for segmenter, tracker in segmenters_and_trackers:\n",
    "            latest_position = tracker.get_latest_position()\n",
    "            if latest_position:\n",
    "                adjusted_position = (latest_position[0] + frame.shape[1], latest_position[1])\n",
    "                draw_positions(combined_frame, [adjusted_position], color=(0, 255, 0))\n",
    "\n",
    "                velocity = tracker.get_latest_velocity()\n",
    "                acceleration = tracker.get_latest_acceleration()\n",
    "                \n",
    "                # Ajustement et dessin des vecteurs sur la frame combinée\n",
    "                velocity_adjusted = (int(velocity[0] * 5), int(velocity[1] * 5))\n",
    "                acceleration_adjusted = (int(acceleration[0] * 0.3), int(acceleration[1] * 0.3))\n",
    "                draw_vectors(combined_frame, adjusted_position, velocity_adjusted, acceleration_adjusted, color_velocity=(255, 0, 0), color_acceleration=(0, 255, 255))\n",
    "                \n",
    "        for _, tracker in segmenters_and_trackers:\n",
    "            impacts = tracker.detect_direction_changes(angle_threshold=30, speed_threshold=15)  # Ajustez les seuils selon vos besoins\n",
    "            for impact_frame in impacts:\n",
    "                if 0 <= impact_frame < len(tracker.positions):\n",
    "                    if frame_count == impact_frame:  # Vérifie si la frame actuelle correspond à un impact\n",
    "                        impact_pos = tracker.positions[impact_frame]\n",
    "                        # Dessiner un point rouge sur l'impact\n",
    "                        cv2.circle(combined_frame, impact_pos, radius=50, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "        # Affichage ou sauvegarde de la frame traitée\n",
    "        if display_mode == 'video':\n",
    "            cv2.imshow('Tracked Objects', combined_frame)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        elif display_mode == 'static' and frame_count < 10:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(cv2.cvtColor(combined_frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    for _ in range(4): cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associez chaque segmenteur à son tracker\n",
    "segmenters_and_trackers = [(left_shoe_segmenter, left_shoe_tracker), (right_shoe_segmenter, right_shoe_tracker), (cap_segmenter, cap_tracker)]\n",
    "\n",
    "# Processus d'exemple\n",
    "process_video(video_path, segmenters_and_trackers, display_mode='video')  # Ou 'video' pour l'affichage vidéo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projetDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
